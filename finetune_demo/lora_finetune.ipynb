{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89b89f64d8f8053d",
   "metadata": {
    "id": "89b89f64d8f8053d"
   },
   "source": [
    "# 单卡GPU 进行 ChatGLM3-6B模型 LORA 高效微调\n",
    "本 Cookbook 将带领开发者使用 `AdvertiseGen` 对 ChatGLM3-6B 数据集进行 lora微调，使其具备专业的广告生成能力。\n",
    "\n",
    "## 硬件需求\n",
    "显存：24GB及以上（推荐使用30系或A10等sm80架构以上的NVIDIA显卡进行尝试）\n",
    "内存：16GB\n",
    "RAM: 2.9 /16 GB\n",
    "GPU RAM: 15.5/16.0 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bd9a514ed09ea6",
   "metadata": {
    "id": "a7bd9a514ed09ea6"
   },
   "source": [
    "## 0. 环境检查\n",
    "首先，先检查代码的运行地址，确保运行地址处于 `finetune_demo` 中。\n",
    "并且，确保已经安装了 `requirements.txt`中的依赖。\n",
    "\n",
    "> 本 demo 中，不需要使用 deepspeed, mpi4py 两个依赖，如果您安装这两个依赖遇到问题，可以不安装这两个依赖。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7703109d1443346",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T05:29:22.200365Z",
     "start_time": "2024-04-14T05:29:22.080929Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-11-20T05:26:52.972261Z",
     "iopub.status.busy": "2024-11-20T05:26:52.971922Z",
     "iopub.status.idle": "2024-11-20T05:26:53.172597Z",
     "shell.execute_reply": "2024-11-20T05:26:53.172016Z",
     "shell.execute_reply.started": "2024-11-20T05:26:52.972233Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/workspace/ChatGLM3/finetune_demo\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f50e92810011977",
   "metadata": {},
   "source": [
    "## 1. 准备数据集\n",
    "我们使用 AdvertiseGen 数据集来进行微调。从 [Google Drive](https://drive.google.com/file/d/13_vf0xRTQsyneRKdD1bZIr93vBGOczrk/view?usp=sharing) 或者 [Tsinghua Cloud](https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1) 下载处理好的 AdvertiseGen 数据集，将解压后的 AdvertiseGen 目录放到本目录的 `/data/` 下, 例如。\n",
    "> /media/zr/Data/Code/ChatGLM3/finetune_demo/data/AdvertiseGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T05:29:23.809255Z",
     "start_time": "2024-04-14T05:29:22.202731Z"
    },
    "ExecutionIndicator": {
     "show": true
    },
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2024-11-20T05:26:54.356644Z",
     "iopub.status.busy": "2024-11-20T05:26:54.356305Z",
     "iopub.status.idle": "2024-11-20T05:26:54.377215Z",
     "shell.execute_reply": "2024-11-20T05:26:54.376702Z",
     "shell.execute_reply.started": "2024-11-20T05:26:54.356622Z"
    },
    "id": "initial_id",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _resolve_path(path: Union[str, Path]) -> Path:\n",
    "    return Path(path).expanduser().resolve()\n",
    "\n",
    "\n",
    "def _mkdir(dir_name: Union[str, Path]):\n",
    "    dir_name = _resolve_path(dir_name)\n",
    "    if not dir_name.is_dir():\n",
    "        dir_name.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "\n",
    "def convert_adgen(data_dir: Union[str, Path], save_dir: Union[str, Path]):\n",
    "    def _convert(in_file: Path, out_file: Path):\n",
    "        _mkdir(out_file.parent)\n",
    "        with open(in_file, encoding='utf-8') as fin:\n",
    "            with open(out_file, 'wt', encoding='utf-8') as fout:\n",
    "                for line in fin:\n",
    "                    dct = json.loads(line)\n",
    "                    sample = {'conversations': [{'role': 'user', 'content': dct['query']},\n",
    "                                                {'role': 'assistant', 'content': dct['SQL']}]}\n",
    "                    fout.write(json.dumps(sample, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    data_dir = _resolve_path(data_dir)\n",
    "    save_dir = _resolve_path(save_dir)\n",
    "\n",
    "    train_file = data_dir / 'train.json'\n",
    "    if train_file.is_file():\n",
    "        out_file = save_dir / train_file.relative_to(data_dir)\n",
    "        _convert(train_file, out_file)\n",
    "\n",
    "    dev_file = data_dir / 'dev.json'\n",
    "    if dev_file.is_file():\n",
    "        out_file = save_dir / dev_file.relative_to(data_dir)\n",
    "        _convert(dev_file, out_file)\n",
    "\n",
    "\n",
    "convert_adgen('data/AdvertiseGen', 'data/AdvertiseGen_fix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b7a99923349056",
   "metadata": {
    "id": "a1b7a99923349056"
   },
   "source": [
    "## 2. 使用命令行开始微调,我们使用 lora 进行微调\n",
    "接着，我们仅需要将配置好的参数以命令行的形式传参给程序，就可以使用命令行进行高效微调。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17c87410a24d844f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T06:23:41.282431Z",
     "start_time": "2024-04-14T05:29:23.810692Z"
    },
    "ExecutionIndicator": {
     "show": true
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-11-20T05:26:57.048833Z",
     "iopub.status.busy": "2024-11-20T05:26:57.048495Z",
     "iopub.status.idle": "2024-11-20T05:54:49.755854Z",
     "shell.execute_reply": "2024-11-20T05:54:49.755290Z",
     "shell.execute_reply.started": "2024-11-20T05:26:57.048799Z"
    },
    "id": "17c87410a24d844f",
    "outputId": "e347fc7d-875e-40c9-c682-3e064100476b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/_distutils_hack/__init__.py:54: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n",
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:21<00:00,  3.11s/it]\n",
      "trainable params: 3,899,392 || all params: 6,247,483,392 || trainable%: 0.06241540401681151\n",
      "--> Model\n",
      "\n",
      "--> model has 3.899392M params\n",
      "\n",
      "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 1152 examples [00:00, 144049.08 examples/s]\n",
      "Setting num_proc from 16 back to 1 for the validation split to disable multiprocessing as it only contains one shard.\n",
      "Generating validation split: 115 examples [00:00, 89989.73 examples/s]\n",
      "Setting num_proc from 16 back to 1 for the test split to disable multiprocessing as it only contains one shard.\n",
      "Generating test split: 115 examples [00:00, 106997.55 examples/s]\n",
      "Map (num_proc=16): 100%|███████████| 1152/1152 [00:00<00:00, 2136.88 examples/s]\n",
      "train_dataset: Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 1152\n",
      "})\n",
      "Map (num_proc=16): 100%|██████████████| 115/115 [00:00<00:00, 221.01 examples/s]\n",
      "val_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 115\n",
      "})\n",
      "Map (num_proc=16): 100%|██████████████| 115/115 [00:00<00:00, 221.78 examples/s]\n",
      "test_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 115\n",
      "})\n",
      "--> Sanity check\n",
      "           '[gMASK]': 64790 -> -100\n",
      "               'sop': 64792 -> -100\n",
      "          '<|user|>': 64795 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '\\n': 13 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '近期': 33329 -> -100\n",
      "               '有哪些': 34953 -> -100\n",
      "               '项目的': 34873 -> -100\n",
      "                '公告': 32689 -> -100\n",
      "                '类型': 33467 -> -100\n",
      "                 '是': 54532 -> -100\n",
      "                 '\"': 30955 -> -100\n",
      "                '中标': 41634 -> -100\n",
      "                '结果': 31951 -> -100\n",
      "                '公示': 34513 -> -100\n",
      "                 '\"': 30955 -> -100\n",
      "                 '？': 31514 -> -100\n",
      "     '<|assistant|>': 64796 -> -100\n",
      "                  '': 30910 -> 30910\n",
      "                '\\n': 13 -> 13\n",
      "            'SELECT': 22592 -> 22592\n",
      "                 '*': 1312 -> 1312\n",
      "              'FROM': 11533 -> 11533\n",
      "                  '': 30910 -> 30910\n",
      "                '项目': 31671 -> 31671\n",
      "                '分类': 33328 -> 33328\n",
      "             'WHERE': 18768 -> 18768\n",
      "                  '': 30910 -> 30910\n",
      "                '公告': 32689 -> 32689\n",
      "                '类型': 33467 -> 33467\n",
      "                \"='\": 6521 -> 6521\n",
      "                '中标': 41634 -> 41634\n",
      "                '结果': 31951 -> 31951\n",
      "                '公示': 34513 -> 34513\n",
      "                 \"'\": 30953 -> 30953\n",
      "                'OR': 6172 -> 6172\n",
      "               'DER': 15930 -> 15930\n",
      "                'BY': 10406 -> 10406\n",
      "                '时间': 47507 -> 47507\n",
      "               'DES': 21709 -> 21709\n",
      "                 'C': 30942 -> 30942\n",
      "               'LIM': 25563 -> 25563\n",
      "                'IT': 1998 -> 1998\n",
      "                  '': 30910 -> 30910\n",
      "                 '1': 30939 -> 30939\n",
      "                 '0': 30940 -> 30940\n",
      "                  '': 2 -> 2\n",
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.19.24, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "***** Running training *****\n",
      "  Num examples = 1,152\n",
      "  Num Epochs = 11\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3,000\n",
      "  Number of trainable parameters = 3,899,392\n",
      "{'loss': 2.151, 'grad_norm': 1.9479751586914062, 'learning_rate': 4.9833333333333336e-05, 'epoch': 0.03}\n",
      "{'loss': 2.1957, 'grad_norm': 1.7365142107009888, 'learning_rate': 4.966666666666667e-05, 'epoch': 0.07}\n",
      "{'loss': 1.7917, 'grad_norm': 2.800795555114746, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.1}\n",
      "{'loss': 1.6788, 'grad_norm': 2.566251039505005, 'learning_rate': 4.933333333333334e-05, 'epoch': 0.14}\n",
      "{'loss': 1.463, 'grad_norm': 2.13728666305542, 'learning_rate': 4.9166666666666665e-05, 'epoch': 0.17}\n",
      "{'loss': 1.1995, 'grad_norm': 2.0449471473693848, 'learning_rate': 4.9e-05, 'epoch': 0.21}\n",
      "{'loss': 1.166, 'grad_norm': 2.5851337909698486, 'learning_rate': 4.883333333333334e-05, 'epoch': 0.24}\n",
      "{'loss': 1.0652, 'grad_norm': 2.23002290725708, 'learning_rate': 4.866666666666667e-05, 'epoch': 0.28}\n",
      "{'loss': 0.8803, 'grad_norm': 2.3389108180999756, 'learning_rate': 4.85e-05, 'epoch': 0.31}\n",
      "{'loss': 0.664, 'grad_norm': 2.9897727966308594, 'learning_rate': 4.8333333333333334e-05, 'epoch': 0.35}\n",
      "{'loss': 0.6937, 'grad_norm': 2.8935835361480713, 'learning_rate': 4.8166666666666674e-05, 'epoch': 0.38}\n",
      "{'loss': 0.431, 'grad_norm': 2.456099510192871, 'learning_rate': 4.8e-05, 'epoch': 0.42}\n",
      "{'loss': 0.4108, 'grad_norm': 2.5662496089935303, 'learning_rate': 4.7833333333333335e-05, 'epoch': 0.45}\n",
      "{'loss': 0.4136, 'grad_norm': 3.0999226570129395, 'learning_rate': 4.766666666666667e-05, 'epoch': 0.49}\n",
      "{'loss': 0.4346, 'grad_norm': 1.8172938823699951, 'learning_rate': 4.75e-05, 'epoch': 0.52}\n",
      "{'loss': 0.3615, 'grad_norm': 2.525357484817505, 'learning_rate': 4.7333333333333336e-05, 'epoch': 0.56}\n",
      "{'loss': 0.3088, 'grad_norm': 4.077320575714111, 'learning_rate': 4.716666666666667e-05, 'epoch': 0.59}\n",
      "{'loss': 0.3523, 'grad_norm': 3.1419975757598877, 'learning_rate': 4.7e-05, 'epoch': 0.62}\n",
      "{'loss': 0.3132, 'grad_norm': 2.8073744773864746, 'learning_rate': 4.683333333333334e-05, 'epoch': 0.66}\n",
      "{'loss': 0.2643, 'grad_norm': 3.135612964630127, 'learning_rate': 4.666666666666667e-05, 'epoch': 0.69}\n",
      "{'loss': 0.34, 'grad_norm': 2.3509304523468018, 'learning_rate': 4.6500000000000005e-05, 'epoch': 0.73}\n",
      "{'loss': 0.2574, 'grad_norm': 2.9397501945495605, 'learning_rate': 4.633333333333333e-05, 'epoch': 0.76}\n",
      "{'loss': 0.2593, 'grad_norm': 2.145078182220459, 'learning_rate': 4.6166666666666666e-05, 'epoch': 0.8}\n",
      "{'loss': 0.217, 'grad_norm': 3.867490768432617, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.83}\n",
      "{'loss': 0.2529, 'grad_norm': 2.4560153484344482, 'learning_rate': 4.5833333333333334e-05, 'epoch': 0.87}\n",
      "{'loss': 0.1472, 'grad_norm': 2.3749516010284424, 'learning_rate': 4.566666666666667e-05, 'epoch': 0.9}\n",
      "{'loss': 0.2369, 'grad_norm': 4.133092403411865, 'learning_rate': 4.55e-05, 'epoch': 0.94}\n",
      "{'loss': 0.2367, 'grad_norm': 2.981083631515503, 'learning_rate': 4.5333333333333335e-05, 'epoch': 0.97}\n",
      "{'loss': 0.1882, 'grad_norm': 3.4848453998565674, 'learning_rate': 4.516666666666667e-05, 'epoch': 1.01}\n",
      "{'loss': 0.1988, 'grad_norm': 1.5091735124588013, 'learning_rate': 4.5e-05, 'epoch': 1.04}\n",
      "{'loss': 0.1988, 'grad_norm': 1.8899403810501099, 'learning_rate': 4.483333333333333e-05, 'epoch': 1.08}\n",
      "{'loss': 0.1512, 'grad_norm': 3.493396282196045, 'learning_rate': 4.466666666666667e-05, 'epoch': 1.11}\n",
      "{'loss': 0.13, 'grad_norm': 2.5736985206604004, 'learning_rate': 4.4500000000000004e-05, 'epoch': 1.15}\n",
      "{'loss': 0.1037, 'grad_norm': 2.381331443786621, 'learning_rate': 4.433333333333334e-05, 'epoch': 1.18}\n",
      "{'loss': 0.1195, 'grad_norm': 0.8508631587028503, 'learning_rate': 4.4166666666666665e-05, 'epoch': 1.22}\n",
      "{'loss': 0.206, 'grad_norm': 4.842858791351318, 'learning_rate': 4.4000000000000006e-05, 'epoch': 1.25}\n",
      "{'loss': 0.133, 'grad_norm': 2.3669474124908447, 'learning_rate': 4.383333333333334e-05, 'epoch': 1.28}\n",
      "{'loss': 0.1488, 'grad_norm': 3.023549795150757, 'learning_rate': 4.3666666666666666e-05, 'epoch': 1.32}\n",
      "{'loss': 0.1737, 'grad_norm': 2.4535422325134277, 'learning_rate': 4.35e-05, 'epoch': 1.35}\n",
      "{'loss': 0.1786, 'grad_norm': 2.420429229736328, 'learning_rate': 4.3333333333333334e-05, 'epoch': 1.39}\n",
      "{'loss': 0.1499, 'grad_norm': 2.5470733642578125, 'learning_rate': 4.316666666666667e-05, 'epoch': 1.42}\n",
      "{'loss': 0.1375, 'grad_norm': 3.649881362915039, 'learning_rate': 4.3e-05, 'epoch': 1.46}\n",
      "{'loss': 0.1313, 'grad_norm': 2.2429487705230713, 'learning_rate': 4.2833333333333335e-05, 'epoch': 1.49}\n",
      "{'loss': 0.2812, 'grad_norm': 3.4756860733032227, 'learning_rate': 4.266666666666667e-05, 'epoch': 1.53}\n",
      "{'loss': 0.1165, 'grad_norm': 2.5257606506347656, 'learning_rate': 4.25e-05, 'epoch': 1.56}\n",
      "{'loss': 0.1289, 'grad_norm': 2.066502094268799, 'learning_rate': 4.233333333333334e-05, 'epoch': 1.6}\n",
      "{'loss': 0.1215, 'grad_norm': 3.4614269733428955, 'learning_rate': 4.216666666666667e-05, 'epoch': 1.63}\n",
      "{'loss': 0.1194, 'grad_norm': 3.7917776107788086, 'learning_rate': 4.2e-05, 'epoch': 1.67}\n",
      "{'loss': 0.1883, 'grad_norm': 2.7193171977996826, 'learning_rate': 4.183333333333334e-05, 'epoch': 1.7}\n",
      "{'loss': 0.1424, 'grad_norm': 3.004523277282715, 'learning_rate': 4.166666666666667e-05, 'epoch': 1.74}\n",
      " 17%|██████▋                                 | 500/3000 [04:18<18:34,  2.24it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:03<00:03,  1.61s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:06<00:02,  2.37s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:08<00:00,  2.20s/it]\u001b[ABuilding prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.701 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "                                                                                \n",
      "\u001b[A{'eval_rouge-1': 91.919916, 'eval_rouge-2': 84.879112, 'eval_rouge-l': 89.05414, 'eval_bleu-4': 0.817651646632401, 'eval_runtime': 13.0967, 'eval_samples_per_second': 3.818, 'eval_steps_per_second': 0.305, 'epoch': 1.74}\n",
      " 17%|██████▋                                 | 500/3000 [04:31<18:34,  2.24it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:09<00:00,  2.20s/it]\u001b[A\n",
      "                                                                                \u001b[ACheckpoint destination directory ./output/checkpoint-500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./output/checkpoint-500\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/workspace/chatglm3-6b - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "{'loss': 0.2152, 'grad_norm': 2.8170478343963623, 'learning_rate': 4.15e-05, 'epoch': 1.77}\n",
      "{'loss': 0.2354, 'grad_norm': 2.368760108947754, 'learning_rate': 4.133333333333333e-05, 'epoch': 1.81}\n",
      "{'loss': 0.1426, 'grad_norm': 2.135735273361206, 'learning_rate': 4.116666666666667e-05, 'epoch': 1.84}\n",
      "{'loss': 0.1016, 'grad_norm': 2.5504677295684814, 'learning_rate': 4.1e-05, 'epoch': 1.88}\n",
      "{'loss': 0.1408, 'grad_norm': 3.600308895111084, 'learning_rate': 4.0833333333333334e-05, 'epoch': 1.91}\n",
      "{'loss': 0.1404, 'grad_norm': 3.546908140182495, 'learning_rate': 4.066666666666667e-05, 'epoch': 1.94}\n",
      "{'loss': 0.1336, 'grad_norm': 3.48932147026062, 'learning_rate': 4.05e-05, 'epoch': 1.98}\n",
      " 19%|███████▋                                | 576/3000 [05:08<26:00,  1.55it/s]/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "{'loss': 0.1163, 'grad_norm': 2.0307717323303223, 'learning_rate': 4.0333333333333336e-05, 'epoch': 2.01}\n",
      "{'loss': 0.1073, 'grad_norm': 1.2641226053237915, 'learning_rate': 4.016666666666667e-05, 'epoch': 2.05}\n",
      "{'loss': 0.0995, 'grad_norm': 2.6020736694335938, 'learning_rate': 4e-05, 'epoch': 2.08}\n",
      "{'loss': 0.0734, 'grad_norm': 3.346771240234375, 'learning_rate': 3.983333333333333e-05, 'epoch': 2.12}\n",
      "{'loss': 0.1334, 'grad_norm': 2.9681761264801025, 'learning_rate': 3.966666666666667e-05, 'epoch': 2.15}\n",
      "{'loss': 0.108, 'grad_norm': 1.8018121719360352, 'learning_rate': 3.9500000000000005e-05, 'epoch': 2.19}\n",
      "{'loss': 0.0568, 'grad_norm': 1.6646798849105835, 'learning_rate': 3.933333333333333e-05, 'epoch': 2.22}\n",
      "{'loss': 0.1197, 'grad_norm': 4.729517459869385, 'learning_rate': 3.9166666666666665e-05, 'epoch': 2.26}\n",
      "{'loss': 0.1195, 'grad_norm': 2.6829450130462646, 'learning_rate': 3.9000000000000006e-05, 'epoch': 2.29}\n",
      "{'loss': 0.1494, 'grad_norm': 3.6858856678009033, 'learning_rate': 3.883333333333333e-05, 'epoch': 2.33}\n",
      "{'loss': 0.143, 'grad_norm': 2.7164275646209717, 'learning_rate': 3.866666666666667e-05, 'epoch': 2.36}\n",
      "{'loss': 0.1363, 'grad_norm': 2.7429568767547607, 'learning_rate': 3.85e-05, 'epoch': 2.4}\n",
      "{'loss': 0.086, 'grad_norm': 1.2266626358032227, 'learning_rate': 3.8333333333333334e-05, 'epoch': 2.43}\n",
      "{'loss': 0.0949, 'grad_norm': 3.0120246410369873, 'learning_rate': 3.816666666666667e-05, 'epoch': 2.47}\n",
      "{'loss': 0.0892, 'grad_norm': 2.370609760284424, 'learning_rate': 3.8e-05, 'epoch': 2.5}\n",
      "{'loss': 0.1326, 'grad_norm': 3.3535032272338867, 'learning_rate': 3.7833333333333336e-05, 'epoch': 2.53}\n",
      "{'loss': 0.0515, 'grad_norm': 2.844482898712158, 'learning_rate': 3.766666666666667e-05, 'epoch': 2.57}\n",
      "{'loss': 0.1357, 'grad_norm': 3.1116225719451904, 'learning_rate': 3.7500000000000003e-05, 'epoch': 2.6}\n",
      "{'loss': 0.0893, 'grad_norm': 1.349306583404541, 'learning_rate': 3.733333333333334e-05, 'epoch': 2.64}\n",
      "{'loss': 0.056, 'grad_norm': 0.6604810357093811, 'learning_rate': 3.7166666666666664e-05, 'epoch': 2.67}\n",
      "{'loss': 0.1125, 'grad_norm': 2.7461886405944824, 'learning_rate': 3.7e-05, 'epoch': 2.71}\n",
      "{'loss': 0.1166, 'grad_norm': 1.364827275276184, 'learning_rate': 3.683333333333334e-05, 'epoch': 2.74}\n",
      "{'loss': 0.148, 'grad_norm': 2.904431104660034, 'learning_rate': 3.6666666666666666e-05, 'epoch': 2.78}\n",
      "{'loss': 0.0957, 'grad_norm': 3.49796724319458, 'learning_rate': 3.65e-05, 'epoch': 2.81}\n",
      "{'loss': 0.1272, 'grad_norm': 2.5866293907165527, 'learning_rate': 3.633333333333333e-05, 'epoch': 2.85}\n",
      "{'loss': 0.0625, 'grad_norm': 3.636795997619629, 'learning_rate': 3.6166666666666674e-05, 'epoch': 2.88}\n",
      "{'loss': 0.0843, 'grad_norm': 3.513885259628296, 'learning_rate': 3.6e-05, 'epoch': 2.92}\n",
      "{'loss': 0.0862, 'grad_norm': 1.656935214996338, 'learning_rate': 3.5833333333333335e-05, 'epoch': 2.95}\n",
      "{'loss': 0.1128, 'grad_norm': 5.321987152099609, 'learning_rate': 3.566666666666667e-05, 'epoch': 2.99}\n",
      "{'loss': 0.0843, 'grad_norm': 2.972738742828369, 'learning_rate': 3.55e-05, 'epoch': 3.02}\n",
      "{'loss': 0.0639, 'grad_norm': 1.430364727973938, 'learning_rate': 3.5333333333333336e-05, 'epoch': 3.06}\n",
      "{'loss': 0.0659, 'grad_norm': 2.8030877113342285, 'learning_rate': 3.516666666666667e-05, 'epoch': 3.09}\n",
      "{'loss': 0.0739, 'grad_norm': 0.8740715980529785, 'learning_rate': 3.5e-05, 'epoch': 3.12}\n",
      "{'loss': 0.0492, 'grad_norm': 2.7357418537139893, 'learning_rate': 3.483333333333334e-05, 'epoch': 3.16}\n",
      "{'loss': 0.097, 'grad_norm': 2.6565706729888916, 'learning_rate': 3.466666666666667e-05, 'epoch': 3.19}\n",
      "{'loss': 0.0791, 'grad_norm': 1.9365681409835815, 'learning_rate': 3.45e-05, 'epoch': 3.23}\n",
      "{'loss': 0.065, 'grad_norm': 2.908620595932007, 'learning_rate': 3.433333333333333e-05, 'epoch': 3.26}\n",
      "{'loss': 0.0701, 'grad_norm': 4.090985298156738, 'learning_rate': 3.4166666666666666e-05, 'epoch': 3.3}\n",
      "{'loss': 0.079, 'grad_norm': 2.0752146244049072, 'learning_rate': 3.4000000000000007e-05, 'epoch': 3.33}\n",
      "{'loss': 0.0441, 'grad_norm': 3.698897123336792, 'learning_rate': 3.3833333333333334e-05, 'epoch': 3.37}\n",
      "{'loss': 0.0469, 'grad_norm': 1.511733889579773, 'learning_rate': 3.366666666666667e-05, 'epoch': 3.4}\n",
      "{'loss': 0.0826, 'grad_norm': 2.8219833374023438, 'learning_rate': 3.35e-05, 'epoch': 3.44}\n",
      "{'loss': 0.0655, 'grad_norm': 1.2674734592437744, 'learning_rate': 3.3333333333333335e-05, 'epoch': 3.47}\n",
      " 33%|█████████████                          | 1000/3000 [08:44<14:49,  2.25it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:03<00:03,  1.61s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:06<00:02,  2.35s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 91.81488000000002, 'eval_rouge-2': 85.717442, 'eval_rouge-l': 89.27327399999999, 'eval_bleu-4': 0.8208691382879336, 'eval_runtime': 12.2363, 'eval_samples_per_second': 4.086, 'eval_steps_per_second': 0.327, 'epoch': 3.47}\n",
      " 33%|█████████████                          | 1000/3000 [08:56<14:49,  2.25it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:08<00:00,  2.21s/it]\u001b[A\n",
      "                                                                                \u001b[ACheckpoint destination directory ./output/checkpoint-1000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./output/checkpoint-1000\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/workspace/chatglm3-6b - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "{'loss': 0.0776, 'grad_norm': 2.840067148208618, 'learning_rate': 3.316666666666667e-05, 'epoch': 3.51}\n",
      "{'loss': 0.0779, 'grad_norm': 0.8449248671531677, 'learning_rate': 3.3e-05, 'epoch': 3.54}\n",
      "{'loss': 0.0573, 'grad_norm': 2.021872043609619, 'learning_rate': 3.283333333333333e-05, 'epoch': 3.58}\n",
      "{'loss': 0.0898, 'grad_norm': 4.400274753570557, 'learning_rate': 3.266666666666667e-05, 'epoch': 3.61}\n",
      "{'loss': 0.0749, 'grad_norm': 2.0826709270477295, 'learning_rate': 3.2500000000000004e-05, 'epoch': 3.65}\n",
      "{'loss': 0.1145, 'grad_norm': 4.8316802978515625, 'learning_rate': 3.233333333333333e-05, 'epoch': 3.68}\n",
      "{'loss': 0.0955, 'grad_norm': 1.6020606756210327, 'learning_rate': 3.2166666666666665e-05, 'epoch': 3.72}\n",
      "{'loss': 0.0305, 'grad_norm': 0.7748323678970337, 'learning_rate': 3.2000000000000005e-05, 'epoch': 3.75}\n",
      "{'loss': 0.0874, 'grad_norm': 4.856330871582031, 'learning_rate': 3.183333333333334e-05, 'epoch': 3.78}\n",
      "{'loss': 0.0583, 'grad_norm': 2.5443248748779297, 'learning_rate': 3.1666666666666666e-05, 'epoch': 3.82}\n",
      "{'loss': 0.0577, 'grad_norm': 5.215391159057617, 'learning_rate': 3.15e-05, 'epoch': 3.85}\n",
      "{'loss': 0.1072, 'grad_norm': 4.037215232849121, 'learning_rate': 3.1333333333333334e-05, 'epoch': 3.89}\n",
      "{'loss': 0.0647, 'grad_norm': 4.380202293395996, 'learning_rate': 3.116666666666667e-05, 'epoch': 3.92}\n",
      "{'loss': 0.0708, 'grad_norm': 2.073957920074463, 'learning_rate': 3.1e-05, 'epoch': 3.96}\n",
      "{'loss': 0.1059, 'grad_norm': 2.46856689453125, 'learning_rate': 3.0833333333333335e-05, 'epoch': 3.99}\n",
      " 38%|██████████████▉                        | 1152/3000 [10:13<19:26,  1.58it/s]/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "{'loss': 0.0364, 'grad_norm': 3.199021816253662, 'learning_rate': 3.066666666666667e-05, 'epoch': 4.03}\n",
      "{'loss': 0.0363, 'grad_norm': 1.7729356288909912, 'learning_rate': 3.05e-05, 'epoch': 4.06}\n",
      "{'loss': 0.0344, 'grad_norm': 2.229149341583252, 'learning_rate': 3.0333333333333337e-05, 'epoch': 4.1}\n",
      "{'loss': 0.0471, 'grad_norm': 2.198681592941284, 'learning_rate': 3.016666666666667e-05, 'epoch': 4.13}\n",
      "{'loss': 0.0419, 'grad_norm': 1.5212063789367676, 'learning_rate': 3e-05, 'epoch': 4.17}\n",
      "{'loss': 0.0557, 'grad_norm': 1.572030782699585, 'learning_rate': 2.9833333333333335e-05, 'epoch': 4.2}\n",
      "{'loss': 0.0976, 'grad_norm': 2.9055328369140625, 'learning_rate': 2.9666666666666672e-05, 'epoch': 4.24}\n",
      "{'loss': 0.0635, 'grad_norm': 1.429185390472412, 'learning_rate': 2.95e-05, 'epoch': 4.27}\n",
      "{'loss': 0.0384, 'grad_norm': 3.00919246673584, 'learning_rate': 2.9333333333333336e-05, 'epoch': 4.31}\n",
      "{'loss': 0.0623, 'grad_norm': 1.8535524606704712, 'learning_rate': 2.916666666666667e-05, 'epoch': 4.34}\n",
      "{'loss': 0.0331, 'grad_norm': 1.4831454753875732, 'learning_rate': 2.9e-05, 'epoch': 4.38}\n",
      "{'loss': 0.0932, 'grad_norm': 4.309304714202881, 'learning_rate': 2.8833333333333334e-05, 'epoch': 4.41}\n",
      "{'loss': 0.0662, 'grad_norm': 5.02840518951416, 'learning_rate': 2.8666666666666668e-05, 'epoch': 4.44}\n",
      "{'loss': 0.0481, 'grad_norm': 1.089186191558838, 'learning_rate': 2.8499999999999998e-05, 'epoch': 4.48}\n",
      "{'loss': 0.0217, 'grad_norm': 1.4404816627502441, 'learning_rate': 2.8333333333333335e-05, 'epoch': 4.51}\n",
      "{'loss': 0.0633, 'grad_norm': 4.718419075012207, 'learning_rate': 2.816666666666667e-05, 'epoch': 4.55}\n",
      "{'loss': 0.0384, 'grad_norm': 2.6521754264831543, 'learning_rate': 2.8000000000000003e-05, 'epoch': 4.58}\n",
      "{'loss': 0.0336, 'grad_norm': 1.4205831289291382, 'learning_rate': 2.7833333333333333e-05, 'epoch': 4.62}\n",
      "{'loss': 0.0462, 'grad_norm': 0.6200821995735168, 'learning_rate': 2.7666666666666667e-05, 'epoch': 4.65}\n",
      "{'loss': 0.0725, 'grad_norm': 0.4065416157245636, 'learning_rate': 2.7500000000000004e-05, 'epoch': 4.69}\n",
      "{'loss': 0.0665, 'grad_norm': 1.5431175231933594, 'learning_rate': 2.733333333333333e-05, 'epoch': 4.72}\n",
      "{'loss': 0.0441, 'grad_norm': 1.0709432363510132, 'learning_rate': 2.716666666666667e-05, 'epoch': 4.76}\n",
      "{'loss': 0.0388, 'grad_norm': 2.245751142501831, 'learning_rate': 2.7000000000000002e-05, 'epoch': 4.79}\n",
      "{'loss': 0.0609, 'grad_norm': 2.499926805496216, 'learning_rate': 2.6833333333333333e-05, 'epoch': 4.83}\n",
      "{'loss': 0.0714, 'grad_norm': 2.2717373371124268, 'learning_rate': 2.6666666666666667e-05, 'epoch': 4.86}\n",
      "{'loss': 0.0573, 'grad_norm': 1.9177134037017822, 'learning_rate': 2.6500000000000004e-05, 'epoch': 4.9}\n",
      "{'loss': 0.0528, 'grad_norm': 2.9729857444763184, 'learning_rate': 2.633333333333333e-05, 'epoch': 4.93}\n",
      "{'loss': 0.0725, 'grad_norm': 3.211066484451294, 'learning_rate': 2.6166666666666668e-05, 'epoch': 4.97}\n",
      "{'loss': 0.0508, 'grad_norm': 2.1983914375305176, 'learning_rate': 2.6000000000000002e-05, 'epoch': 5.0}\n",
      "{'loss': 0.063, 'grad_norm': 1.058220386505127, 'learning_rate': 2.5833333333333336e-05, 'epoch': 5.03}\n",
      "{'loss': 0.0243, 'grad_norm': 1.0849344730377197, 'learning_rate': 2.5666666666666666e-05, 'epoch': 5.07}\n",
      "{'loss': 0.0181, 'grad_norm': 0.36379459500312805, 'learning_rate': 2.5500000000000003e-05, 'epoch': 5.1}\n",
      "{'loss': 0.0308, 'grad_norm': 1.7958266735076904, 'learning_rate': 2.5333333333333337e-05, 'epoch': 5.14}\n",
      "{'loss': 0.039, 'grad_norm': 0.6911312937736511, 'learning_rate': 2.5166666666666667e-05, 'epoch': 5.17}\n",
      "{'loss': 0.043, 'grad_norm': 2.0123748779296875, 'learning_rate': 2.5e-05, 'epoch': 5.21}\n",
      " 50%|███████████████████▌                   | 1500/3000 [13:13<10:32,  2.37it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:03<00:03,  1.62s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:06<00:02,  2.39s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 92.89935799999999, 'eval_rouge-2': 86.81889200000002, 'eval_rouge-l': 89.83993599999998, 'eval_bleu-4': 0.8376901152268833, 'eval_runtime': 11.8779, 'eval_samples_per_second': 4.209, 'eval_steps_per_second': 0.337, 'epoch': 5.21}\n",
      " 50%|███████████████████▌                   | 1500/3000 [13:25<10:32,  2.37it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:08<00:00,  2.08s/it]\u001b[A\n",
      "                                                                                \u001b[ACheckpoint destination directory ./output/checkpoint-1500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./output/checkpoint-1500\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/workspace/chatglm3-6b - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "{'loss': 0.0384, 'grad_norm': 3.308582305908203, 'learning_rate': 2.4833333333333335e-05, 'epoch': 5.24}\n",
      "{'loss': 0.0308, 'grad_norm': 3.200423240661621, 'learning_rate': 2.466666666666667e-05, 'epoch': 5.28}\n",
      "{'loss': 0.0389, 'grad_norm': 1.8683851957321167, 'learning_rate': 2.45e-05, 'epoch': 5.31}\n",
      "{'loss': 0.0176, 'grad_norm': 1.9152755737304688, 'learning_rate': 2.4333333333333336e-05, 'epoch': 5.35}\n",
      "{'loss': 0.0641, 'grad_norm': 2.358091115951538, 'learning_rate': 2.4166666666666667e-05, 'epoch': 5.38}\n",
      "{'loss': 0.0513, 'grad_norm': 4.3660888671875, 'learning_rate': 2.4e-05, 'epoch': 5.42}\n",
      "{'loss': 0.0436, 'grad_norm': 0.37240344285964966, 'learning_rate': 2.3833333333333334e-05, 'epoch': 5.45}\n",
      "{'loss': 0.024, 'grad_norm': 3.2217860221862793, 'learning_rate': 2.3666666666666668e-05, 'epoch': 5.49}\n",
      "{'loss': 0.0285, 'grad_norm': 4.058572292327881, 'learning_rate': 2.35e-05, 'epoch': 5.52}\n",
      "{'loss': 0.0453, 'grad_norm': 1.8372466564178467, 'learning_rate': 2.3333333333333336e-05, 'epoch': 5.56}\n",
      "{'loss': 0.0337, 'grad_norm': 3.1947648525238037, 'learning_rate': 2.3166666666666666e-05, 'epoch': 5.59}\n",
      "{'loss': 0.0212, 'grad_norm': 0.36977851390838623, 'learning_rate': 2.3000000000000003e-05, 'epoch': 5.62}\n",
      "{'loss': 0.0561, 'grad_norm': 2.195366382598877, 'learning_rate': 2.2833333333333334e-05, 'epoch': 5.66}\n",
      "{'loss': 0.0419, 'grad_norm': 4.283124923706055, 'learning_rate': 2.2666666666666668e-05, 'epoch': 5.69}\n",
      "{'loss': 0.0636, 'grad_norm': 8.1240816116333, 'learning_rate': 2.25e-05, 'epoch': 5.73}\n",
      "{'loss': 0.0467, 'grad_norm': 2.3072526454925537, 'learning_rate': 2.2333333333333335e-05, 'epoch': 5.76}\n",
      "{'loss': 0.0337, 'grad_norm': 1.9802253246307373, 'learning_rate': 2.216666666666667e-05, 'epoch': 5.8}\n",
      "{'loss': 0.037, 'grad_norm': 2.101947784423828, 'learning_rate': 2.2000000000000003e-05, 'epoch': 5.83}\n",
      "{'loss': 0.0223, 'grad_norm': 0.6210219264030457, 'learning_rate': 2.1833333333333333e-05, 'epoch': 5.87}\n",
      "{'loss': 0.0324, 'grad_norm': 1.5872244834899902, 'learning_rate': 2.1666666666666667e-05, 'epoch': 5.9}\n",
      "{'loss': 0.0518, 'grad_norm': 4.0836992263793945, 'learning_rate': 2.15e-05, 'epoch': 5.94}\n",
      "{'loss': 0.0339, 'grad_norm': 1.5765472650527954, 'learning_rate': 2.1333333333333335e-05, 'epoch': 5.97}\n",
      " 58%|██████████████████████▍                | 1728/3000 [15:19<11:33,  1.83it/s]/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "{'loss': 0.0532, 'grad_norm': 3.174773931503296, 'learning_rate': 2.116666666666667e-05, 'epoch': 6.01}\n",
      "{'loss': 0.0212, 'grad_norm': 0.2304232269525528, 'learning_rate': 2.1e-05, 'epoch': 6.04}\n",
      "{'loss': 0.0157, 'grad_norm': 2.0595524311065674, 'learning_rate': 2.0833333333333336e-05, 'epoch': 6.08}\n",
      "{'loss': 0.0457, 'grad_norm': 1.593920111656189, 'learning_rate': 2.0666666666666666e-05, 'epoch': 6.11}\n",
      "{'loss': 0.0232, 'grad_norm': 2.986828088760376, 'learning_rate': 2.05e-05, 'epoch': 6.15}\n",
      "{'loss': 0.0186, 'grad_norm': 1.497183084487915, 'learning_rate': 2.0333333333333334e-05, 'epoch': 6.18}\n",
      "{'loss': 0.0269, 'grad_norm': 0.9283789992332458, 'learning_rate': 2.0166666666666668e-05, 'epoch': 6.22}\n",
      "{'loss': 0.0267, 'grad_norm': 0.9697421789169312, 'learning_rate': 2e-05, 'epoch': 6.25}\n",
      "{'loss': 0.0196, 'grad_norm': 0.290621280670166, 'learning_rate': 1.9833333333333335e-05, 'epoch': 6.28}\n",
      "{'loss': 0.0283, 'grad_norm': 3.097241163253784, 'learning_rate': 1.9666666666666666e-05, 'epoch': 6.32}\n",
      "{'loss': 0.0395, 'grad_norm': 3.628004550933838, 'learning_rate': 1.9500000000000003e-05, 'epoch': 6.35}\n",
      "{'loss': 0.0273, 'grad_norm': 1.4696959257125854, 'learning_rate': 1.9333333333333333e-05, 'epoch': 6.39}\n",
      "{'loss': 0.0384, 'grad_norm': 2.604614734649658, 'learning_rate': 1.9166666666666667e-05, 'epoch': 6.42}\n",
      "{'loss': 0.0221, 'grad_norm': 1.1184035539627075, 'learning_rate': 1.9e-05, 'epoch': 6.46}\n",
      "{'loss': 0.0282, 'grad_norm': 0.6150358319282532, 'learning_rate': 1.8833333333333335e-05, 'epoch': 6.49}\n",
      "{'loss': 0.0186, 'grad_norm': 0.6799616813659668, 'learning_rate': 1.866666666666667e-05, 'epoch': 6.53}\n",
      "{'loss': 0.028, 'grad_norm': 1.7793282270431519, 'learning_rate': 1.85e-05, 'epoch': 6.56}\n",
      "{'loss': 0.0392, 'grad_norm': 2.6635584831237793, 'learning_rate': 1.8333333333333333e-05, 'epoch': 6.6}\n",
      "{'loss': 0.0188, 'grad_norm': 0.2934364974498749, 'learning_rate': 1.8166666666666667e-05, 'epoch': 6.63}\n",
      "{'loss': 0.0186, 'grad_norm': 3.7081406116485596, 'learning_rate': 1.8e-05, 'epoch': 6.67}\n",
      "{'loss': 0.0166, 'grad_norm': 3.3925273418426514, 'learning_rate': 1.7833333333333334e-05, 'epoch': 6.7}\n",
      "{'loss': 0.0245, 'grad_norm': 0.7252643704414368, 'learning_rate': 1.7666666666666668e-05, 'epoch': 6.74}\n",
      "{'loss': 0.0313, 'grad_norm': 1.9887114763259888, 'learning_rate': 1.75e-05, 'epoch': 6.77}\n",
      "{'loss': 0.0714, 'grad_norm': 4.539589881896973, 'learning_rate': 1.7333333333333336e-05, 'epoch': 6.81}\n",
      "{'loss': 0.0237, 'grad_norm': 1.4853850603103638, 'learning_rate': 1.7166666666666666e-05, 'epoch': 6.84}\n",
      "{'loss': 0.0361, 'grad_norm': 1.9825057983398438, 'learning_rate': 1.7000000000000003e-05, 'epoch': 6.88}\n",
      "{'loss': 0.022, 'grad_norm': 4.621834754943848, 'learning_rate': 1.6833333333333334e-05, 'epoch': 6.91}\n",
      "{'loss': 0.0272, 'grad_norm': 0.49470892548561096, 'learning_rate': 1.6666666666666667e-05, 'epoch': 6.94}\n",
      " 67%|██████████████████████████             | 2000/3000 [17:37<07:43,  2.16it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:03<00:03,  1.62s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:06<00:02,  2.39s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 92.89683799999999, 'eval_rouge-2': 86.66624, 'eval_rouge-l': 89.74119400000001, 'eval_bleu-4': 0.8303609640351098, 'eval_runtime': 12.5277, 'eval_samples_per_second': 3.991, 'eval_steps_per_second': 0.319, 'epoch': 6.94}\n",
      " 67%|██████████████████████████             | 2000/3000 [17:50<07:43,  2.16it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:08<00:00,  2.19s/it]\u001b[A\n",
      "                                                                                \u001b[ACheckpoint destination directory ./output/checkpoint-2000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./output/checkpoint-2000\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/workspace/chatglm3-6b - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "{'loss': 0.0131, 'grad_norm': 0.5754241347312927, 'learning_rate': 1.65e-05, 'epoch': 6.98}\n",
      " 67%|██████████████████████████▏            | 2016/3000 [17:59<11:08,  1.47it/s]/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "{'loss': 0.0172, 'grad_norm': 1.7090208530426025, 'learning_rate': 1.6333333333333335e-05, 'epoch': 7.01}\n",
      "{'loss': 0.0259, 'grad_norm': 1.2749642133712769, 'learning_rate': 1.6166666666666665e-05, 'epoch': 7.05}\n",
      "{'loss': 0.0134, 'grad_norm': 1.9820244312286377, 'learning_rate': 1.6000000000000003e-05, 'epoch': 7.08}\n",
      "{'loss': 0.0485, 'grad_norm': 2.5968897342681885, 'learning_rate': 1.5833333333333333e-05, 'epoch': 7.12}\n",
      "{'loss': 0.031, 'grad_norm': 0.7503426671028137, 'learning_rate': 1.5666666666666667e-05, 'epoch': 7.15}\n",
      "{'loss': 0.0172, 'grad_norm': 1.6721969842910767, 'learning_rate': 1.55e-05, 'epoch': 7.19}\n",
      "{'loss': 0.0172, 'grad_norm': 3.7316737174987793, 'learning_rate': 1.5333333333333334e-05, 'epoch': 7.22}\n",
      "{'loss': 0.0129, 'grad_norm': 1.717768907546997, 'learning_rate': 1.5166666666666668e-05, 'epoch': 7.26}\n",
      "{'loss': 0.0099, 'grad_norm': 0.14879320561885834, 'learning_rate': 1.5e-05, 'epoch': 7.29}\n",
      "{'loss': 0.0173, 'grad_norm': 1.7257964611053467, 'learning_rate': 1.4833333333333336e-05, 'epoch': 7.33}\n",
      "{'loss': 0.0106, 'grad_norm': 2.2015116214752197, 'learning_rate': 1.4666666666666668e-05, 'epoch': 7.36}\n",
      "{'loss': 0.019, 'grad_norm': 5.868487358093262, 'learning_rate': 1.45e-05, 'epoch': 7.4}\n",
      "{'loss': 0.0169, 'grad_norm': 1.020696759223938, 'learning_rate': 1.4333333333333334e-05, 'epoch': 7.43}\n",
      "{'loss': 0.0217, 'grad_norm': 0.6489571332931519, 'learning_rate': 1.4166666666666668e-05, 'epoch': 7.47}\n",
      "{'loss': 0.0189, 'grad_norm': 0.5133687853813171, 'learning_rate': 1.4000000000000001e-05, 'epoch': 7.5}\n",
      "{'loss': 0.0101, 'grad_norm': 0.3313961923122406, 'learning_rate': 1.3833333333333334e-05, 'epoch': 7.53}\n",
      "{'loss': 0.0136, 'grad_norm': 1.805659294128418, 'learning_rate': 1.3666666666666666e-05, 'epoch': 7.57}\n",
      "{'loss': 0.0191, 'grad_norm': 1.4424521923065186, 'learning_rate': 1.3500000000000001e-05, 'epoch': 7.6}\n",
      "{'loss': 0.0265, 'grad_norm': 0.5806852579116821, 'learning_rate': 1.3333333333333333e-05, 'epoch': 7.64}\n",
      "{'loss': 0.0213, 'grad_norm': 0.23955583572387695, 'learning_rate': 1.3166666666666665e-05, 'epoch': 7.67}\n",
      "{'loss': 0.0147, 'grad_norm': 1.750525712966919, 'learning_rate': 1.3000000000000001e-05, 'epoch': 7.71}\n",
      "{'loss': 0.0119, 'grad_norm': 0.28828293085098267, 'learning_rate': 1.2833333333333333e-05, 'epoch': 7.74}\n",
      "{'loss': 0.0122, 'grad_norm': 4.236119747161865, 'learning_rate': 1.2666666666666668e-05, 'epoch': 7.78}\n",
      "{'loss': 0.0334, 'grad_norm': 1.6181020736694336, 'learning_rate': 1.25e-05, 'epoch': 7.81}\n",
      "{'loss': 0.0422, 'grad_norm': 3.3561346530914307, 'learning_rate': 1.2333333333333334e-05, 'epoch': 7.85}\n",
      "{'loss': 0.0067, 'grad_norm': 0.8617352843284607, 'learning_rate': 1.2166666666666668e-05, 'epoch': 7.88}\n",
      "{'loss': 0.0302, 'grad_norm': 0.6986038088798523, 'learning_rate': 1.2e-05, 'epoch': 7.92}\n",
      "{'loss': 0.0232, 'grad_norm': 0.6346147656440735, 'learning_rate': 1.1833333333333334e-05, 'epoch': 7.95}\n",
      "{'loss': 0.0161, 'grad_norm': 2.2376954555511475, 'learning_rate': 1.1666666666666668e-05, 'epoch': 7.99}\n",
      "{'loss': 0.0193, 'grad_norm': 0.7692263126373291, 'learning_rate': 1.1500000000000002e-05, 'epoch': 8.02}\n",
      "{'loss': 0.0187, 'grad_norm': 2.2860805988311768, 'learning_rate': 1.1333333333333334e-05, 'epoch': 8.06}\n",
      "{'loss': 0.0163, 'grad_norm': 2.1431031227111816, 'learning_rate': 1.1166666666666668e-05, 'epoch': 8.09}\n",
      "{'loss': 0.0105, 'grad_norm': 0.4614175260066986, 'learning_rate': 1.1000000000000001e-05, 'epoch': 8.12}\n",
      "{'loss': 0.0082, 'grad_norm': 2.0408413410186768, 'learning_rate': 1.0833333333333334e-05, 'epoch': 8.16}\n",
      "{'loss': 0.024, 'grad_norm': 1.0846657752990723, 'learning_rate': 1.0666666666666667e-05, 'epoch': 8.19}\n",
      "{'loss': 0.0081, 'grad_norm': 0.14724834263324738, 'learning_rate': 1.05e-05, 'epoch': 8.23}\n",
      "{'loss': 0.0104, 'grad_norm': 1.432401180267334, 'learning_rate': 1.0333333333333333e-05, 'epoch': 8.26}\n",
      "{'loss': 0.0091, 'grad_norm': 1.8509809970855713, 'learning_rate': 1.0166666666666667e-05, 'epoch': 8.3}\n",
      "{'loss': 0.0134, 'grad_norm': 2.4097108840942383, 'learning_rate': 1e-05, 'epoch': 8.33}\n",
      "{'loss': 0.0112, 'grad_norm': 0.320798397064209, 'learning_rate': 9.833333333333333e-06, 'epoch': 8.37}\n",
      "{'loss': 0.021, 'grad_norm': 0.9612046480178833, 'learning_rate': 9.666666666666667e-06, 'epoch': 8.4}\n",
      "{'loss': 0.011, 'grad_norm': 3.8402035236358643, 'learning_rate': 9.5e-06, 'epoch': 8.44}\n",
      "{'loss': 0.0089, 'grad_norm': 0.9411811232566833, 'learning_rate': 9.333333333333334e-06, 'epoch': 8.47}\n",
      "{'loss': 0.015, 'grad_norm': 0.4499809145927429, 'learning_rate': 9.166666666666666e-06, 'epoch': 8.51}\n",
      "{'loss': 0.01, 'grad_norm': 0.810033917427063, 'learning_rate': 9e-06, 'epoch': 8.54}\n",
      "{'loss': 0.0148, 'grad_norm': 0.6100162267684937, 'learning_rate': 8.833333333333334e-06, 'epoch': 8.58}\n",
      "{'loss': 0.0137, 'grad_norm': 1.3492485284805298, 'learning_rate': 8.666666666666668e-06, 'epoch': 8.61}\n",
      "{'loss': 0.0061, 'grad_norm': 0.7303667068481445, 'learning_rate': 8.500000000000002e-06, 'epoch': 8.65}\n",
      "{'loss': 0.0139, 'grad_norm': 0.681688666343689, 'learning_rate': 8.333333333333334e-06, 'epoch': 8.68}\n",
      " 83%|████████████████████████████████▌      | 2500/3000 [22:03<03:48,  2.18it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:03<00:03,  1.62s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:06<00:02,  2.39s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 93.563546, 'eval_rouge-2': 87.730202, 'eval_rouge-l': 90.27705399999999, 'eval_bleu-4': 0.8389424046585131, 'eval_runtime': 12.3001, 'eval_samples_per_second': 4.065, 'eval_steps_per_second': 0.325, 'epoch': 8.68}\n",
      " 83%|████████████████████████████████▌      | 2500/3000 [22:16<03:48,  2.18it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:08<00:00,  2.11s/it]\u001b[A\n",
      "                                                                                \u001b[ACheckpoint destination directory ./output/checkpoint-2500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./output/checkpoint-2500\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/workspace/chatglm3-6b - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "{'loss': 0.0221, 'grad_norm': 2.9309775829315186, 'learning_rate': 8.166666666666668e-06, 'epoch': 8.72}\n",
      "{'loss': 0.0146, 'grad_norm': 1.0007368326187134, 'learning_rate': 8.000000000000001e-06, 'epoch': 8.75}\n",
      "{'loss': 0.0164, 'grad_norm': 1.2396894693374634, 'learning_rate': 7.833333333333333e-06, 'epoch': 8.78}\n",
      "{'loss': 0.0172, 'grad_norm': 1.5265672206878662, 'learning_rate': 7.666666666666667e-06, 'epoch': 8.82}\n",
      "{'loss': 0.0576, 'grad_norm': 3.2019050121307373, 'learning_rate': 7.5e-06, 'epoch': 8.85}\n",
      "{'loss': 0.0158, 'grad_norm': 1.056329369544983, 'learning_rate': 7.333333333333334e-06, 'epoch': 8.89}\n",
      "{'loss': 0.0151, 'grad_norm': 2.669661521911621, 'learning_rate': 7.166666666666667e-06, 'epoch': 8.92}\n",
      "{'loss': 0.0087, 'grad_norm': 0.37146127223968506, 'learning_rate': 7.000000000000001e-06, 'epoch': 8.96}\n",
      "{'loss': 0.0152, 'grad_norm': 2.9430758953094482, 'learning_rate': 6.833333333333333e-06, 'epoch': 8.99}\n",
      " 86%|█████████████████████████████████▋     | 2592/3000 [23:04<03:55,  1.73it/s]/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "{'loss': 0.0157, 'grad_norm': 0.9104992747306824, 'learning_rate': 6.666666666666667e-06, 'epoch': 9.03}\n",
      "{'loss': 0.0078, 'grad_norm': 0.6511356234550476, 'learning_rate': 6.5000000000000004e-06, 'epoch': 9.06}\n",
      "{'loss': 0.0095, 'grad_norm': 0.2007925808429718, 'learning_rate': 6.333333333333334e-06, 'epoch': 9.1}\n",
      "{'loss': 0.0118, 'grad_norm': 3.4261043071746826, 'learning_rate': 6.166666666666667e-06, 'epoch': 9.13}\n",
      "{'loss': 0.0037, 'grad_norm': 0.5986480712890625, 'learning_rate': 6e-06, 'epoch': 9.17}\n",
      "{'loss': 0.0355, 'grad_norm': 5.249936103820801, 'learning_rate': 5.833333333333334e-06, 'epoch': 9.2}\n",
      "{'loss': 0.0092, 'grad_norm': 1.6901215314865112, 'learning_rate': 5.666666666666667e-06, 'epoch': 9.24}\n",
      "{'loss': 0.0061, 'grad_norm': 0.562943696975708, 'learning_rate': 5.500000000000001e-06, 'epoch': 9.27}\n",
      "{'loss': 0.0067, 'grad_norm': 2.2487010955810547, 'learning_rate': 5.333333333333334e-06, 'epoch': 9.31}\n",
      "{'loss': 0.0071, 'grad_norm': 0.588224470615387, 'learning_rate': 5.166666666666667e-06, 'epoch': 9.34}\n",
      "{'loss': 0.0049, 'grad_norm': 0.7075811624526978, 'learning_rate': 5e-06, 'epoch': 9.38}\n",
      "{'loss': 0.009, 'grad_norm': 2.416131019592285, 'learning_rate': 4.833333333333333e-06, 'epoch': 9.41}\n",
      "{'loss': 0.0133, 'grad_norm': 1.3708336353302002, 'learning_rate': 4.666666666666667e-06, 'epoch': 9.44}\n",
      "{'loss': 0.0132, 'grad_norm': 3.0610406398773193, 'learning_rate': 4.5e-06, 'epoch': 9.48}\n",
      "{'loss': 0.0104, 'grad_norm': 0.43653973937034607, 'learning_rate': 4.333333333333334e-06, 'epoch': 9.51}\n",
      "{'loss': 0.013, 'grad_norm': 0.3109678030014038, 'learning_rate': 4.166666666666667e-06, 'epoch': 9.55}\n",
      "{'loss': 0.0179, 'grad_norm': 3.800305128097534, 'learning_rate': 4.000000000000001e-06, 'epoch': 9.58}\n",
      "{'loss': 0.0103, 'grad_norm': 0.23875363171100616, 'learning_rate': 3.833333333333334e-06, 'epoch': 9.62}\n",
      "{'loss': 0.0057, 'grad_norm': 0.1833217293024063, 'learning_rate': 3.666666666666667e-06, 'epoch': 9.65}\n",
      "{'loss': 0.0058, 'grad_norm': 0.193353071808815, 'learning_rate': 3.5000000000000004e-06, 'epoch': 9.69}\n",
      "{'loss': 0.0112, 'grad_norm': 2.7122106552124023, 'learning_rate': 3.3333333333333333e-06, 'epoch': 9.72}\n",
      "{'loss': 0.0066, 'grad_norm': 0.7372493147850037, 'learning_rate': 3.166666666666667e-06, 'epoch': 9.76}\n",
      "{'loss': 0.0174, 'grad_norm': 2.381105422973633, 'learning_rate': 3e-06, 'epoch': 9.79}\n",
      "{'loss': 0.007, 'grad_norm': 0.8504496216773987, 'learning_rate': 2.8333333333333335e-06, 'epoch': 9.83}\n",
      "{'loss': 0.0099, 'grad_norm': 1.816834807395935, 'learning_rate': 2.666666666666667e-06, 'epoch': 9.86}\n",
      "{'loss': 0.0264, 'grad_norm': 2.2490785121917725, 'learning_rate': 2.5e-06, 'epoch': 9.9}\n",
      "{'loss': 0.0102, 'grad_norm': 0.5724769830703735, 'learning_rate': 2.3333333333333336e-06, 'epoch': 9.93}\n",
      "{'loss': 0.0124, 'grad_norm': 0.7948729991912842, 'learning_rate': 2.166666666666667e-06, 'epoch': 9.97}\n",
      "{'loss': 0.008, 'grad_norm': 0.2662014961242676, 'learning_rate': 2.0000000000000003e-06, 'epoch': 10.0}\n",
      "{'loss': 0.009, 'grad_norm': 2.1128551959991455, 'learning_rate': 1.8333333333333335e-06, 'epoch': 10.03}\n",
      "{'loss': 0.0072, 'grad_norm': 0.7161373496055603, 'learning_rate': 1.6666666666666667e-06, 'epoch': 10.07}\n",
      "{'loss': 0.0051, 'grad_norm': 0.1678566187620163, 'learning_rate': 1.5e-06, 'epoch': 10.1}\n",
      "{'loss': 0.0057, 'grad_norm': 0.22954441606998444, 'learning_rate': 1.3333333333333334e-06, 'epoch': 10.14}\n",
      "{'loss': 0.0058, 'grad_norm': 2.7292165756225586, 'learning_rate': 1.1666666666666668e-06, 'epoch': 10.17}\n",
      "{'loss': 0.0117, 'grad_norm': 2.368501663208008, 'learning_rate': 1.0000000000000002e-06, 'epoch': 10.21}\n",
      "{'loss': 0.0109, 'grad_norm': 0.15307728946208954, 'learning_rate': 8.333333333333333e-07, 'epoch': 10.24}\n",
      "{'loss': 0.0064, 'grad_norm': 0.4209514260292053, 'learning_rate': 6.666666666666667e-07, 'epoch': 10.28}\n",
      "{'loss': 0.0047, 'grad_norm': 0.3458356559276581, 'learning_rate': 5.000000000000001e-07, 'epoch': 10.31}\n",
      "{'loss': 0.0176, 'grad_norm': 0.7412222623825073, 'learning_rate': 3.3333333333333335e-07, 'epoch': 10.35}\n",
      "{'loss': 0.0349, 'grad_norm': 1.5019413232803345, 'learning_rate': 1.6666666666666668e-07, 'epoch': 10.38}\n",
      "{'loss': 0.0035, 'grad_norm': 0.5118281245231628, 'learning_rate': 0.0, 'epoch': 10.42}\n",
      "100%|███████████████████████████████████████| 3000/3000 [26:33<00:00,  1.99it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:03<00:03,  1.62s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:06<00:02,  2.41s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 93.932232, 'eval_rouge-2': 88.345738, 'eval_rouge-l': 90.970916, 'eval_bleu-4': 0.8434813707231773, 'eval_runtime': 12.3048, 'eval_samples_per_second': 4.063, 'eval_steps_per_second': 0.325, 'epoch': 10.42}\n",
      "100%|███████████████████████████████████████| 3000/3000 [26:46<00:00,  1.99it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:08<00:00,  2.11s/it]\u001b[A\n",
      "                                                                                \u001b[ACheckpoint destination directory ./output/checkpoint-3000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./output/checkpoint-3000\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/workspace/chatglm3-6b - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1606.521, 'train_samples_per_second': 7.47, 'train_steps_per_second': 1.867, 'train_loss': 0.11613419612248739, 'epoch': 10.42}\n",
      "100%|███████████████████████████████████████| 3000/3000 [26:46<00:00,  1.87it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "***** Running Prediction *****\n",
      "  Num examples = 115\n",
      "  Batch size = 16\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:19<00:00,  2.43s/it]\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 NCCL_P2P_DISABLE=\"1\" NCCL_IB_DISABLE=\"1\" python finetune_hf.py  data/AdvertiseGen_fix  /mnt/workspace/chatglm3-6b  configs/lora.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9418f6c5c264601",
   "metadata": {
    "id": "d9418f6c5c264601"
   },
   "source": [
    "## 3. 使用微调的数据集进行推理\n",
    "在完成微调任务之后，我们可以查看到 `output` 文件夹下多了很多个`checkpoint-*`的文件夹，这些文件夹代表了训练的轮数。\n",
    "我们选择最后一轮的微调权重，并使用inference进行导入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5060015c24e97ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T06:23:52.725227Z",
     "start_time": "2024-04-14T06:23:41.284552Z"
    },
    "ExecutionIndicator": {
     "show": true
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-11-20T06:07:02.130449Z",
     "iopub.status.busy": "2024-11-20T06:07:02.130081Z",
     "iopub.status.idle": "2024-11-20T06:07:15.723558Z",
     "shell.execute_reply": "2024-11-20T06:07:15.722892Z",
     "shell.execute_reply.started": "2024-11-20T06:07:02.130418Z"
    },
    "id": "5060015c24e97ae",
    "outputId": "d3f03d0d-46bf-4c74-9b00-dc0160da0e15",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/_distutils_hack/__init__.py:54: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:02<00:00,  2.54it/s]\n",
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n",
      "SELECT 中标人名称,COUNT(中标人名称) AS nums FROM 中标结果 GROUP BY 中标人名称 ORDER BY nums DESC LIMIT 1\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 NCCL_P2P_DISABLE=\"1\" NCCL_IB_DISABLE=\"1\" python inference_hf.py output/checkpoint-3000/ --prompt \"哪家公司中标数量最多\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cd83087f096094",
   "metadata": {
    "id": "18cd83087f096094"
   },
   "source": [
    "## 4. 总结\n",
    "到此位置，我们就完成了使用单张 GPU Lora 来微调 ChatGLM3-6B 模型，使其能生产出更好的广告。\n",
    "在本章节中，你将会学会：\n",
    "+ 如何使用模型进行 Lora 微调\n",
    "+ 微调数据集的准备和对齐\n",
    "+ 使用微调的模型进行推理"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
